import json
import tensorflow as tf
import numpy as np

RNN_UNITS = 1024
EMBEDDING_DIM = 256

MAX_PREDICTION = 1000  # Maximum characters generated by the model
SENTENCE_SEPARATORS = '\n.'


def build_model(vocab_size, batch_size):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM,
                                  batch_input_shape=[batch_size, None]),
        tf.keras.layers.LSTM(RNN_UNITS,
                             return_sequences=True,
                             stateful=True,
                             recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)
    ])
    return model


def load_encoding(enc_file):
    with open(enc_file) as file:
        vocab = json.load(file)
    return vocab


class Model:
    def __init__(self, encoding_file, weights_file):
        self.vocab = load_encoding(encoding_file)
        self.char2idx = {u:i for i, u in enumerate(self.vocab)}
        self.idx2char = np.array(self.vocab)
        self.weights_file = weights_file

    def generate_text(self, starting_string, num_sentences):
        task_model = self.__load_model__()
        return self.__generate_text__(task_model, starting_string, num_sentences)

    def __generate_text__(self, model, start_string, num_sentences):
        input_eval = [self.char2idx[s] for s in start_string]
        input_eval = tf.expand_dims(input_eval, 0)
        text_generated = []

        temperature = 0.8

        sentences_generated = 0

        for _ in range(MAX_PREDICTION):
            if sentences_generated >= num_sentences:
                break

            predictions = model(input_eval)

            # remove the batch dimension
            predictions = tf.squeeze(predictions, 0)

            # using a multinomial distribution to predict the word returned by the model
            predictions = predictions / temperature
            predicted_id = tf.multinomial(predictions, num_samples=1)[-1, 0].numpy()

            # We pass the predicted word as the next input to the model
            # along with the previous hidden state
            input_eval = tf.expand_dims([predicted_id], 0)
            crt_char = self.idx2char[predicted_id]
            text_generated.append(crt_char)

            if crt_char in SENTENCE_SEPARATORS:
                sentences_generated = sentences_generated + 1

        return ''.join(text_generated)

    def __load_model__(self):
        model = build_model(
            len(self.vocab),
            batch_size=1
        )
        model.load_weights(self.weights_file)
        model.build(tf.TensorShape([1, None]))
        return model
